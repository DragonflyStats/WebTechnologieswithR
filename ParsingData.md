Tools for Working with the Web from R 
========================================
Parsing Data from the Web 
• downloading files : download.file() is in base R and commonly used way to download a file. However, downloading files over HTTPS is not supported in R's internal method for download.file(). The download() function in the package downloader wraps download.file(), and takes all the same arguments, but works for https across platforms. 
• tabular data as txt, csv, etc. : You can use read.table(), read.csv(), and friends to read a table directly from a URL, or after acquiring the csv file from the web via e.g., getURL() from RCurl. read.csv() works with http but not https, i.e.: read.csv("http://..."), but not read.csv("https://..."). You can download a file first before reading the file in R, and you can use downloader to download over https. read.table() and friends also have a text parameter so you can read a table if a table is encoded as a string with line breaks, etc. 
• JSON I/O : JSON is javascript object notation . There are three packages for reading and writing JSON: rjson, RJSONIO, and jsonlite. jsonlite includes a different parser from RJSONIO called yajl . We recommend using jsonlite. Check out the paper describing jsonlite by Jeroen Ooms http://arxiv.org/abs/1403.2805 . 
• XML/HTML I/O : The package XML contains functions for parsing XML and HTML, and supports xpath for searching XML (think regex for strings). A helpful function to read data from one or more HTML tables is readHTMLTable(). XML also includes XPATH parsing ability, see xpathApply() and xpathSApply(). The XML2R package is a collection of convenient functions for coercing XML into data frames (development version on GitHub ). An alternative to XML is selectr , which parses CSS3 Selectors and translates them to XPath 1.0 expressions. XML package is often used for parsing xml and html, but selectr translates CSS selectors to XPath, so can use the CSS selectors instead of XPath. The selectorgadget browser extension can be used to identify page elements. RHTMLForms reads HTML documents and obtains a description of each of the forms it contains, along with the different elements and hidden fields. scrapeR provides additional tools for scraping data from HTML and XML documents. htmltab extracts structured information from HTML tables, similar to XML::readHTMLTable of the XML package, but automatically expands row and column spans in the header and body cells, and users are given more control over the identification of header and body rows which will end up in the R table. 
• rvest : rvest scrapes html from web pages, and is designed to work with magrittr to make it easy to express common web scraping tasks. 
• The tldextract package extract top level domains and subdomains from a host name. It's a port of a Python library of the same name . 
• webutils: Utility functions for developing web applications. Parsers for application/x-www-form-urlencoded as well as multipart/form-data. Source on Github 
• urltools: URL encoding, decoding, parsing, and parameter extraction. Source on Github 
• The repmis package contains a source_data() command to load and cache plain-text data from a URL (either http or https). It also includes source_Dropbox() for downloading/caching plain-text data from non-public Dropbox folders and source_XlsxData() for downloading/caching Excel xlsx sheets. 
• rsdmx provides tools to read data and metadata documents exchanged through the Statistical Data and Metadata Exchange (SDMX) framework. The package currently focuses on the SDMX XML standard format (SDMX-ML). project website (Github) .
 
